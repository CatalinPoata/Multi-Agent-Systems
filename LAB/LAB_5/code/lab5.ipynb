{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T06:35:18.273909Z",
     "start_time": "2025-04-04T06:35:14.453453Z"
    }
   },
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:35:19.621669Z",
     "start_time": "2025-04-04T06:35:19.616671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPSILONS = [0.1, 0.5, 0.9]\n",
    "GAMMAS = [0.5, 0.9]\n",
    "LEARNING_RATES = [0.1, 0.5, 0.9]\n",
    "EPISODES = 50000\n",
    "CHECKPOINT_EVERY = 50\n",
    "EXPERIMENT_TRIES = 50\n",
    "MAX_STEPS = 1000"
   ],
   "id": "adf3dc6f889c4837",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:35:23.105872Z",
     "start_time": "2025-04-04T06:35:23.054284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def epsilon_greedy(env, s, q, epsilon):\n",
    "    prob = np.random.rand()\n",
    "    if prob < epsilon:\n",
    "        # print('Exploration')\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # print(\"Exploitation\")\n",
    "        return np.argmax([q[s, a] for a in range(env.action_space.n)])\n",
    "\n",
    "\n",
    "def cosine_annealing(episode, total_episodes, epsilon_max=1.0, epsilon_min=0.01):\n",
    "    return epsilon_min + 0.5 * (epsilon_max - epsilon_min) * (1 + np.cos(np.pi * episode / total_episodes))\n",
    "\n",
    "\n",
    "\n",
    "def q_learning(env, gamma, epsilon, learning_rate):\n",
    "    states = env.observation_space.n\n",
    "    actions = env.action_space.n\n",
    "\n",
    "    q = np.zeros((states, actions))\n",
    "\n",
    "    print(q)\n",
    "    print(q.shape)\n",
    "    average_rewards = []\n",
    "\n",
    "    for episode in range(EPISODES):\n",
    "        print('Episode:', episode)\n",
    "        curr_epsilon = cosine_annealing(episode, EPISODES, epsilon)\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "        cur_steps = 0\n",
    "        while not terminated and cur_steps < MAX_STEPS:\n",
    "            action = epsilon_greedy(env, state, q, curr_epsilon)\n",
    "            next_state, reward, terminated, _, _= env.step(action)\n",
    "            q[state, action] = q[state, action] + learning_rate * (reward + gamma * max(q[next_state, :]) - q[state, action])\n",
    "            state = next_state\n",
    "            cur_steps += 1\n",
    "\n",
    "        if episode % CHECKPOINT_EVERY == 0:\n",
    "            print(f\"Current epsilon: {curr_epsilon}\")\n",
    "            sum_rewards = 0\n",
    "            policy = np.zeros(states)\n",
    "            for s in range(states):\n",
    "                policy[s] = np.argmax(q[s, :])\n",
    "\n",
    "            for run in range(EXPERIMENT_TRIES):\n",
    "                # print(f\"Start run: {run}\")\n",
    "                run_state, _ = env.reset()\n",
    "                terminated_run = False\n",
    "                run_steps = 0\n",
    "                while not terminated_run and run_steps < MAX_STEPS:\n",
    "                    run_action = policy[run_state]\n",
    "                    next_run_state, run_reward, terminated_run, _, _ = env.step(run_action)\n",
    "                    sum_rewards += run_reward\n",
    "                    run_state = next_run_state\n",
    "                    run_steps += 1\n",
    "                # print(f\"Finish run: {run}\")\n",
    "\n",
    "            average_reward = sum_rewards / EXPERIMENT_TRIES\n",
    "            average_rewards.append(average_reward)\n",
    "\n",
    "        print(f\"Done episode {episode}\")\n",
    "    pi = np.zeros(states)\n",
    "    for s in range(states):\n",
    "        pi[s] = np.argmax(q[s, :])\n",
    "    return pi, average_rewards\n",
    "\n",
    "def sarsa(env, gamma, epsilon, learning_rate):\n",
    "    states = env.observation_space.n\n",
    "    actions = env.action_space.n\n",
    "\n",
    "    q = np.zeros((states, actions))\n",
    "    print(q)\n",
    "    print(q.shape)\n",
    "    average_rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        print('Episode:', episode)\n",
    "        curr_epsilon = cosine_annealing(episode, EPISODES, epsilon)\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "        curr_steps = 0\n",
    "        action = epsilon_greedy(env, state, q, curr_epsilon)\n",
    "        while not terminated and curr_steps < MAX_STEPS:\n",
    "            next_state, reward, terminated, _, _ = env.step(action)\n",
    "            action_prime = epsilon_greedy(env, next_state, q, curr_epsilon)\n",
    "            q[state, action] = q[state, action] + learning_rate * (reward + gamma * q[next_state, action_prime] - q[state, action])\n",
    "            state = next_state\n",
    "            action = action_prime\n",
    "            curr_steps += 1\n",
    "\n",
    "        if episode % CHECKPOINT_EVERY == 0:\n",
    "            print(f\"Current epsilon: {curr_epsilon}\")\n",
    "            sum_rewards = 0\n",
    "            policy = np.zeros(states)\n",
    "            for s in range(states):\n",
    "                policy[s] = np.argmax(q[s, :])\n",
    "\n",
    "            for run in range(EXPERIMENT_TRIES):\n",
    "                run_state, _ = env.reset()\n",
    "                terminated_run = False\n",
    "                run_steps = 0\n",
    "                while not terminated_run and run_steps < MAX_STEPS:\n",
    "                    run_action = policy[run_state]\n",
    "                    next_run_state, run_reward, terminated_run, _, _ = env.step(run_action)\n",
    "                    sum_rewards += run_reward\n",
    "                    run_state = next_run_state\n",
    "                    run_steps += 1\n",
    "\n",
    "            average_reward = sum_rewards / EXPERIMENT_TRIES\n",
    "            average_rewards.append(average_reward)\n",
    "        print(f\"Done episode {episode}\")\n",
    "\n",
    "    pi = np.zeros(states)\n",
    "    for s in range(states):\n",
    "        pi[s] = np.argmax(q[s, :])\n",
    "\n",
    "    return pi, average_rewards\n",
    "\n",
    "def compute_data():\n",
    "    envs = [gymnasium.make('Taxi-v3'), gymnasium.make('FrozenLake-v1')]\n",
    "    results = {}\n",
    "    for env in envs:\n",
    "        for gamma in GAMMAS:\n",
    "            for learning_rate in LEARNING_RATES:\n",
    "                for epsilon in EPSILONS:\n",
    "                    q_policy, q_rewards = q_learning(env, gamma, epsilon, learning_rate)\n",
    "                    s_policy, s_rewards = sarsa(env, gamma, epsilon, learning_rate)\n",
    "                    key = (env.spec.id, gamma, learning_rate, epsilon)\n",
    "\n",
    "                    results[key] = {\n",
    "                        'q_rewards': q_rewards,\n",
    "                        's_rewards': s_rewards,\n",
    "                        'q_policy': q_policy,\n",
    "                        's_policy': s_policy,\n",
    "                        'q_max_reward': np.max(q_rewards),\n",
    "                        's_max_reward': np.max(s_rewards)\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_experiments(data):\n",
    "    envs = [gymnasium.make('Taxi-v3'), gymnasium.make('FrozenLake-v1')]\n",
    "    for env in envs:\n",
    "        for gamma in GAMMAS:\n",
    "            for learning_rate in LEARNING_RATES:\n",
    "                plt.figure()\n",
    "                for epsilon in EPSILONS:\n",
    "                    key = (env.spec.id, gamma, learning_rate, epsilon)\n",
    "                    q_rewards = data[key]['q_rewards']\n",
    "                    s_rewards = data[key]['s_rewards']\n",
    "                    max_q = data[key]['q_max_reward']\n",
    "                    max_s = data[key]['s_max_reward']\n",
    "                    plt.plot(q_rewards, label=f'Q-Learning Rewards for Epsilon = {epsilon}')\n",
    "                    plt.plot(s_rewards, label=f'SARSA Rewards for Epsilon = {epsilon}')\n",
    "                    plt.scatter(q_rewards.index(max_q), max_q, color='blue', marker='o', label=f'Max Q-Learning Reward for Epsilon = {epsilon}')\n",
    "                    plt.scatter(s_rewards.index(max_s), max_s, color='red', marker='o', label=f'Max SARSA Reward for Epsilon = {epsilon}')\n",
    "                plt.xlabel('Checkpoint')\n",
    "                plt.ylabel('Reward')\n",
    "                plt.title(f\"γ={gamma}, α={learning_rate}\")\n",
    "                plt.legend()\n",
    "                plt.savefig(f\"../results/{key[0]}/G{GAMMAS.index(gamma)}_LR{LEARNING_RATES.index(learning_rate)}.png\")\n",
    "                plt.close()\n",
    "\n",
    "    for env in envs:\n",
    "        for epsilon in EPSILONS:\n",
    "            for learning_rate in LEARNING_RATES:\n",
    "                plt.figure()\n",
    "                for gamma in GAMMAS:\n",
    "                    key = (env.spec.id, gamma, learning_rate, epsilon)\n",
    "                    q_rewards = data[key]['q_rewards']\n",
    "                    s_rewards = data[key]['s_rewards']\n",
    "                    max_q = data[key]['q_max_reward']\n",
    "                    max_s = data[key]['s_max_reward']\n",
    "                    plt.plot(q_rewards, label=f'Q-Learning Rewards for Gamma = {gamma}')\n",
    "                    plt.plot(s_rewards, label=f'SARSA Rewards for Gamma = {gamma}')\n",
    "                    plt.scatter(q_rewards.index(max_q), max_q, color='blue', marker='o', label=f'Max Q-Learning Reward for Gamma = {gamma}')\n",
    "                    plt.scatter(s_rewards.index(max_s), max_s, color='red', marker='o', label=f'Max SARSA Reward for Gamma = {gamma}')\n",
    "                plt.xlabel('Checkpoint')\n",
    "                plt.ylabel('Reward')\n",
    "                plt.title(f\"ε={epsilon}, α={learning_rate}\")\n",
    "                plt.legend()\n",
    "                plt.savefig(f\"../results/{key[0]}/E{EPSILONS.index(epsilon)}_LR{LEARNING_RATES.index(learning_rate)}.png\")\n",
    "                plt.close()\n",
    "\n",
    "    for env in envs:\n",
    "        for epsilon in EPSILONS:\n",
    "            for gamma in GAMMAS:\n",
    "                plt.figure()\n",
    "                for learning_rate in LEARNING_RATES:\n",
    "                    key = (env.spec.id, gamma, learning_rate, epsilon)\n",
    "                    q_rewards = data[key]['q_rewards']\n",
    "                    s_rewards = data[key]['s_rewards']\n",
    "                    max_q = data[key]['q_max_reward']\n",
    "                    max_s = data[key]['s_max_reward']\n",
    "                    plt.plot(q_rewards, label=f'Q-Learning Rewards for Learning Rate = {learning_rate}')\n",
    "                    plt.plot(s_rewards, label=f'SARSA Rewards for Learning Rate = {learning_rate}')\n",
    "                    plt.scatter(q_rewards.index(max_q), max_q, color='blue', marker='o', label=f'Max Q-Learning Reward for Learning Rate = {learning_rate}')\n",
    "                    plt.scatter(s_rewards.index(max_s), max_s, color='red', marker='o', label=f'Max SARSA Reward for Learning Rate = {learning_rate}')\n",
    "                plt.xlabel('Checkpoint')\n",
    "                plt.ylabel('Reward')\n",
    "                plt.title(f\"ε={epsilon}, γ={gamma}\")\n",
    "                plt.legend()\n",
    "                plt.savefig(f\"../results/{key[0]}/E{EPSILONS.index(epsilon)}_G{GAMMAS.index(gamma)}.png\")\n",
    "                plt.close()\n"
   ],
   "id": "bff7c46ce88e1d2e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-04T06:35:27.714604Z"
    }
   },
   "cell_type": "code",
   "source": "data = compute_data()",
   "id": "7748a863b42fe1c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "(500, 6)\n",
      "Episode: 0\n",
      "Current epsilon: 0.1\n",
      "Done episode 0\n",
      "Episode: 1\n",
      "Done episode 1\n",
      "Episode: 2\n",
      "Done episode 2\n",
      "Episode: 3\n",
      "Done episode 3\n",
      "Episode: 4\n",
      "Done episode 4\n",
      "Episode: 5\n",
      "Done episode 5\n",
      "Episode: 6\n",
      "Done episode 6\n",
      "Episode: 7\n",
      "Done episode 7\n",
      "Episode: 8\n",
      "Done episode 8\n",
      "Episode: 9\n",
      "Done episode 9\n",
      "Episode: 10\n",
      "Done episode 10\n",
      "Episode: 11\n",
      "Done episode 11\n",
      "Episode: 12\n",
      "Done episode 12\n",
      "Episode: 13\n",
      "Done episode 13\n",
      "Episode: 14\n",
      "Done episode 14\n",
      "Episode: 15\n",
      "Done episode 15\n",
      "Episode: 16\n",
      "Done episode 16\n",
      "Episode: 17\n",
      "Done episode 17\n",
      "Episode: 18\n",
      "Done episode 18\n",
      "Episode: 19\n",
      "Done episode 19\n",
      "Episode: 20\n",
      "Done episode 20\n",
      "Episode: 21\n",
      "Done episode 21\n",
      "Episode: 22\n",
      "Done episode 22\n",
      "Episode: 23\n",
      "Done episode 23\n",
      "Episode: 24\n",
      "Done episode 24\n",
      "Episode: 25\n",
      "Done episode 25\n",
      "Episode: 26\n",
      "Done episode 26\n",
      "Episode: 27\n",
      "Done episode 27\n",
      "Episode: 28\n",
      "Done episode 28\n",
      "Episode: 29\n",
      "Done episode 29\n",
      "Episode: 30\n",
      "Done episode 30\n",
      "Episode: 31\n",
      "Done episode 31\n",
      "Episode: 32\n",
      "Done episode 32\n",
      "Episode: 33\n",
      "Done episode 33\n",
      "Episode: 34\n",
      "Done episode 34\n",
      "Episode: 35\n",
      "Done episode 35\n",
      "Episode: 36\n",
      "Done episode 36\n",
      "Episode: 37\n",
      "Done episode 37\n",
      "Episode: 38\n",
      "Done episode 38\n",
      "Episode: 39\n",
      "Done episode 39\n",
      "Episode: 40\n",
      "Done episode 40\n",
      "Episode: 41\n",
      "Done episode 41\n",
      "Episode: 42\n",
      "Done episode 42\n",
      "Episode: 43\n",
      "Done episode 43\n",
      "Episode: 44\n",
      "Done episode 44\n",
      "Episode: 45\n",
      "Done episode 45\n",
      "Episode: 46\n",
      "Done episode 46\n",
      "Episode: 47\n",
      "Done episode 47\n",
      "Episode: 48\n",
      "Done episode 48\n",
      "Episode: 49\n",
      "Done episode 49\n",
      "Episode: 50\n",
      "Current epsilon: 0.09999977793408363\n",
      "Done episode 50\n",
      "Episode: 51\n",
      "Done episode 51\n",
      "Episode: 52\n",
      "Done episode 52\n",
      "Episode: 53\n",
      "Done episode 53\n",
      "Episode: 54\n",
      "Done episode 54\n",
      "Episode: 55\n",
      "Done episode 55\n",
      "Episode: 56\n",
      "Done episode 56\n",
      "Episode: 57\n",
      "Done episode 57\n",
      "Episode: 58\n",
      "Done episode 58\n",
      "Episode: 59\n",
      "Done episode 59\n",
      "Episode: 60\n",
      "Done episode 60\n",
      "Episode: 61\n",
      "Done episode 61\n",
      "Episode: 62\n",
      "Done episode 62\n",
      "Episode: 63\n",
      "Done episode 63\n",
      "Episode: 64\n",
      "Done episode 64\n",
      "Episode: 65\n",
      "Done episode 65\n",
      "Episode: 66\n",
      "Done episode 66\n",
      "Episode: 67\n",
      "Done episode 67\n",
      "Episode: 68\n",
      "Done episode 68\n",
      "Episode: 69\n",
      "Done episode 69\n",
      "Episode: 70\n",
      "Done episode 70\n",
      "Episode: 71\n",
      "Done episode 71\n",
      "Episode: 72\n",
      "Done episode 72\n",
      "Episode: 73\n",
      "Done episode 73\n",
      "Episode: 74\n",
      "Done episode 74\n",
      "Episode: 75\n",
      "Done episode 75\n",
      "Episode: 76\n",
      "Done episode 76\n",
      "Episode: 77\n",
      "Done episode 77\n",
      "Episode: 78\n",
      "Done episode 78\n",
      "Episode: 79\n",
      "Done episode 79\n",
      "Episode: 80\n",
      "Done episode 80\n",
      "Episode: 81\n",
      "Done episode 81\n",
      "Episode: 82\n",
      "Done episode 82\n",
      "Episode: 83\n",
      "Done episode 83\n",
      "Episode: 84\n",
      "Done episode 84\n",
      "Episode: 85\n",
      "Done episode 85\n",
      "Episode: 86\n",
      "Done episode 86\n",
      "Episode: 87\n",
      "Done episode 87\n",
      "Episode: 88\n",
      "Done episode 88\n",
      "Episode: 89\n",
      "Done episode 89\n",
      "Episode: 90\n",
      "Done episode 90\n",
      "Episode: 91\n",
      "Done episode 91\n",
      "Episode: 92\n",
      "Done episode 92\n",
      "Episode: 93\n",
      "Done episode 93\n",
      "Episode: 94\n",
      "Done episode 94\n",
      "Episode: 95\n",
      "Done episode 95\n",
      "Episode: 96\n",
      "Done episode 96\n",
      "Episode: 97\n",
      "Done episode 97\n",
      "Episode: 98\n",
      "Done episode 98\n",
      "Episode: 99\n",
      "Done episode 99\n",
      "Episode: 100\n",
      "Current epsilon: 0.09999911173852617\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "run_experiments(data)",
   "id": "8adb420f3bc87c26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
